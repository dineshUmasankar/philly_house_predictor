# %%
import pandas as pd

df = pd.read_csv('scaled.csv')
df = df.drop(columns='Unnamed: 0') # This was an index column generated by accident
df.corrwith(df['market_value_capped']).to_csv('all_corrs.csv', header=True)

correlations = df.corrwith(df['market_value_capped'])

strong_correlations = correlations[abs(correlations) > 0.20]
strong_correlations = strong_correlations.sort_values()

# Select TOP 10 Positive Features
top_10_features = strong_correlations.tail(11) # Includes market_value_capped which we will exclude
print(top_10_features)

# %%
selected_features = df[['number_stories', 'depth_capped', 'exempt_building_encoded', 'total_area_capped', 'total_livable_area_capped', 'exempt_building_capped', 'frontage_capped', 'zip_code_0', 'taxable_building_capped', 'taxable_land_capped', 'market_value_capped']]
selected_features.to_csv('high_correlations.csv')

# %%
from sklearn.decomposition import PCA

# The dataset is already scaled so we can apply PCA

target_variable = df['market_value_capped']
df_pca = df.drop(columns='market_value_capped')
# Showcase explained variance of PCA up to all possible principal components
pca = PCA(n_components=10)
pca_all = pca.fit_transform(df_pca)
print(pca.explained_variance_ratio_)
sum(pca.explained_variance_ratio_)
pca_df = pd.DataFrame(pca_all, columns=['PCA_1', 'PCA_2', 'PCA_3', 'PCA_4', 'PCA_5', 'PCA_6', 'PCA_7', 'PCA_8', 'PCA_9', 'PCA_10'])
pca_df = pca_df.join(target_variable)
pca_df.to_csv('pca_10component.csv')


