{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dines\\AppData\\Local\\Temp\\ipykernel_40632\\384017758.py:114: DtypeWarning: Columns (5,26,77) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv('original_dataset.csv')\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def drop_high_missing_percent_columns(df):\n",
    "    # Drop columns with more than 25% missing values\n",
    "    df = df.dropna(thresh=len(df) * 0.25, axis=1)\n",
    "    return df\n",
    "\n",
    "def drop_high_cardinality_columns(df):\n",
    "    # Drop columns with too many distinct values as we want to form clusters within the dataset\n",
    "    # and create relationships between the features. These distinct values within a single column for\n",
    "    # every single record blurs the relationships, and as such we are removing it.\n",
    "    # The entire dataset has about 580k rows at this point, so we are targeting columns with 20% distinct values (116k distinct values)\n",
    "    high_cardinality_columns = [col for col in df.columns if df[col].nunique() > 116_000]\n",
    "    # The high_cardinality columns are: ['the_geom', 'the_geom_webmercator', 'book_and_page', 'location', 'parcel_number', 'registry_number', 'pin', 'objectid', 'lat', 'lng']\n",
    "    # We want to preserve lat,lng so we remove it from the list\n",
    "    df = df.drop(columns=high_cardinality_columns)\n",
    "    return df\n",
    "\n",
    "def filter_single_multifamily_homes(df):\n",
    "    # Filter out only homes (single / multi-family homes)\n",
    "    df = df[(df['category_code_description'] == \"SINGLE FAMILY\")]\n",
    "    return df\n",
    "\n",
    "def drop_specific(df):\n",
    "    # Dropped in order to avoid recency bias\n",
    "    df = df.drop(columns=['assessment_date'])\n",
    "\n",
    "    # There's only a single category: Single Family Home(s) and both of these columns have redundant information of the same single value representing single-family home(s).\n",
    "    df = df.drop(columns=['category_code'])\n",
    "    df = df.drop(columns=['category_code_description'])\n",
    "\n",
    "    # This column only re-assures us if the year built has been estimated and doesn't really provide us value towards estimating an valuation of the property.\n",
    "    # More valuable alternative is the year_built column which also has much less missing values.\n",
    "    df = df.drop(columns=['year_built_estimate'])\n",
    "\n",
    "    # Dropped in order to avoid recency bias\n",
    "    df = df.drop(columns=['recording_date'])\n",
    "    # Drop columns: 'mailing_city_state', 'mailing_zip' b/c the mailing address / state of a property owner doesn't indicate a property's worth.\n",
    "    df = df.drop(columns=['mailing_city_state', 'mailing_zip'])\n",
    "\n",
    "    # Dropped building_code because it was not possible to find a clear definition of what the codes had represented.\n",
    "    # There is no updated code manual nor is it even described within the metadata of the Office of Phildelphia's Assessment \n",
    "    df = df.drop(columns=['building_code'])\n",
    "\n",
    "    # Dropped b/c water department uses this as some form of identification number that was not clarified by the Office of Philadelphia's metadata.\n",
    "    df = df.drop(columns=['street_code'])\n",
    "\n",
    "    # Too many missing values and near impossible to impute. It is simply a nominal attribute that is hard to attribute towards our target variable and very susceptible\n",
    "    # to forming bad patterns within our model such as north direction could increase market valuation, but the zipcode and lat/lng are better indicators of an area based\n",
    "    # valuation.\n",
    "    df = df.drop(columns=['street_direction'])\n",
    "    \n",
    "    # Unknown definition of two-digit numbers, weren't even listed in the OPA's metadata\n",
    "    df = df.drop(columns=['building_code_new'])\n",
    "\n",
    "    # Remove all Vacant Land Properties from the dataset\n",
    "    df = df[~df['building_code_description'].str.contains(\"VACANT\", regex=False, na=False)]\n",
    "\n",
    "    # Remove records with placeholder values for sale_price and market_value\n",
    "    df = df[df['sale_price'] > 1]\n",
    "    df = df[df['market_value'] > 1]\n",
    "    \n",
    "    # Have ensured at this point most of the dataset is primarily of single-family homes.\n",
    "    df = df.drop(columns=['building_code_description'])\n",
    "    df = df.drop(columns=['building_code_description_new'])\n",
    "    \n",
    "    # Removed Central Air as it's a binary variable with no other meaningful reference to impute from (38% missing)\n",
    "    df = df.drop(columns=['central_air'])\n",
    "\n",
    "    # Unclear Meaning from Metadata\n",
    "    df = df.drop(columns=['off_street_open'])\n",
    "\n",
    "    # Dropping State Code as this information isn't pertinent to relations of the market value within Philadelphia\n",
    "    df = df.drop(columns=['state_code'])\n",
    "\n",
    "    # Dropping House Number as these are based more on the local neighborhoods within Philadelphia which tends to have very little value in regards to market_value\n",
    "    df = df.drop(columns=['house_number'])\n",
    "\n",
    "    # Nearly 12k missing values and there's unclear definition on what is an average and what each letter represents for general construction from the metadata\n",
    "    df = df.drop(columns=['general_construction'])\n",
    "\n",
    "    # Unclear Definitions and too many messy values\n",
    "    df = df.drop(columns=['quality_grade'])\n",
    "\n",
    "    # Dropped due to low correlation to market value: 0.06\n",
    "    df = df.drop(columns=['exempt_land'])\n",
    "\n",
    "    # Dropped due to too wide spread when manually analyzing\n",
    "    df = df.drop(columns=['sale_price'])\n",
    "\n",
    "    return df\n",
    "\n",
    "def impute_columns(df):\n",
    "    # Imputed Missing Values in basement column with new attribute K.\n",
    "    df = df.fillna({'basements': \"K\"})\n",
    "\n",
    "    # Imputed Missing Values in type_heated column with attribute H (represents missing/unknown heating type for property).\n",
    "    df = df.fillna({'type_heater': \"H\"})\n",
    "\n",
    "    # Imputed Missing Values in topography column with attribute F (represents street level as most properties in philadelphia are at street level according OPA).\n",
    "    df = df.fillna({'topography': \"F\"})\n",
    "\n",
    "    return df\n",
    "\n",
    "def drop_missing_vals_records(df):\n",
    "    df = df.dropna(subset=['census_tract', 'depth', 'exterior_condition', 'fireplaces', 'frontage', 'garage_spaces', \n",
    "                           'geographic_ward', 'interior_condition', 'market_value', 'number_of_bathrooms', 'number_of_bedrooms', \n",
    "                           'number_stories', 'parcel_shape', 'taxable_building', 'total_area', 'total_livable_area', 'view_type',\n",
    "                            'year_built', 'zip_code', 'zoning'])\n",
    "\n",
    "    return df\n",
    "    \n",
    "# Load original_dataset from Office of Property Assessments\n",
    "df = pd.read_csv('original_dataset.csv')\n",
    "\n",
    "df_clean_missing = drop_high_missing_percent_columns(df.copy())\n",
    "df_clean_high_cardinality = drop_high_cardinality_columns(df_clean_missing.copy())\n",
    "df_filter_single_family_homes = filter_single_multifamily_homes(df_clean_high_cardinality.copy())\n",
    "df_remove_specific = drop_specific(df_filter_single_family_homes.copy())\n",
    "df_impute_columns = impute_columns(df_remove_specific.copy())\n",
    "df_remove_missing_vals_records = drop_missing_vals_records(df_impute_columns.copy())\n",
    "\n",
    "# Define a function to filter dates before 2024\n",
    "def filter_dates(row):\n",
    "    year = int(row['sale_date'][:4])\n",
    "    return year < 2024\n",
    "\n",
    "# Apply the filter function to the DataFrame and drop the column as we want to avoid recency bias\n",
    "df_filter_saledate = df_remove_missing_vals_records[df_remove_missing_vals_records.apply(filter_dates, axis=1)].drop(columns=['sale_date'])\n",
    "\n",
    "def filter_specific(df):\n",
    "    # Constraining Basements to the following valid indexes and transform to ordinal encoding\n",
    "    valid_basement = ['0', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K']\n",
    "    df = df[df['basements'].isin(valid_basement)]\n",
    "\n",
    "    # Constraining type_heater to valid definitions from metadata\n",
    "    valid_type_heater = ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H']\n",
    "    df = df[df['type_heater'].isin(valid_type_heater)]\n",
    "\n",
    "    # # Constraining interior condition based on valid definitions from metadata\n",
    "    # # Drop interior condition with 0/1's as it means not applicable / unknown definition\n",
    "    # valid_interior_condition = ['2', '3', '4', '5', '6', '7']\n",
    "    # df = df[df['interior_condition'].isin(valid_interior_condition)]\n",
    "    return df\n",
    "\n",
    "\n",
    "df_filter_specific = filter_specific(df_filter_saledate.copy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Expected a 2-dimensional container but got <class 'pandas.core.series.Series'> instead. Pass a DataFrame containing a single row (i.e. single sample) or a single column (i.e. single feature) instead.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[81], line 7\u001b[0m\n\u001b[0;32m      5\u001b[0m valid_basements \u001b[39m=\u001b[39m [\u001b[39m'\u001b[39m\u001b[39m0\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mA\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mB\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mC\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mD\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mE\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mF\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mG\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mH\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mI\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mJ\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mK\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[0;32m      6\u001b[0m basements_encoder \u001b[39m=\u001b[39m OrdinalEncoder(categories\u001b[39m=\u001b[39m[valid_basements])\n\u001b[1;32m----> 7\u001b[0m df_encode[\u001b[39m'\u001b[39m\u001b[39mbasements_encoded\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m basements_encoder\u001b[39m.\u001b[39;49mfit_transform(df_encode[\u001b[39m'\u001b[39;49m\u001b[39mbasements\u001b[39;49m\u001b[39m'\u001b[39;49m])\n\u001b[0;32m      8\u001b[0m df_encode \u001b[39m=\u001b[39m df_encode\u001b[39m.\u001b[39mdrop(columns\u001b[39m=\u001b[39m[\u001b[39m'\u001b[39m\u001b[39mbasements\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[0;32m     10\u001b[0m \u001b[39m# # Exterior Condition Encoding\u001b[39;00m\n\u001b[0;32m     11\u001b[0m \u001b[39m# valid_exterior_conditions = sorted(df_encode['exterior_condition'].unique().astype(int), reverse=True)\u001b[39;00m\n\u001b[0;32m     12\u001b[0m \u001b[39m# exterior_encoder = OrdinalEncoder(categories=[valid_exterior_conditions])\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     27\u001b[0m \n\u001b[0;32m     28\u001b[0m \u001b[39m# df_encode.to_csv('fil.csv')\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\sklearn\\utils\\_set_output.py:295\u001b[0m, in \u001b[0;36m_wrap_method_output.<locals>.wrapped\u001b[1;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[0;32m    293\u001b[0m \u001b[39m@wraps\u001b[39m(f)\n\u001b[0;32m    294\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mwrapped\u001b[39m(\u001b[39mself\u001b[39m, X, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m--> 295\u001b[0m     data_to_wrap \u001b[39m=\u001b[39m f(\u001b[39mself\u001b[39;49m, X, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m    296\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(data_to_wrap, \u001b[39mtuple\u001b[39m):\n\u001b[0;32m    297\u001b[0m         \u001b[39m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[0;32m    298\u001b[0m         return_tuple \u001b[39m=\u001b[39m (\n\u001b[0;32m    299\u001b[0m             _wrap_data_with_container(method, data_to_wrap[\u001b[39m0\u001b[39m], X, \u001b[39mself\u001b[39m),\n\u001b[0;32m    300\u001b[0m             \u001b[39m*\u001b[39mdata_to_wrap[\u001b[39m1\u001b[39m:],\n\u001b[0;32m    301\u001b[0m         )\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\sklearn\\base.py:1098\u001b[0m, in \u001b[0;36mTransformerMixin.fit_transform\u001b[1;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[0;32m   1083\u001b[0m         warnings\u001b[39m.\u001b[39mwarn(\n\u001b[0;32m   1084\u001b[0m             (\n\u001b[0;32m   1085\u001b[0m                 \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mThis object (\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m) has a `transform`\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1093\u001b[0m             \u001b[39mUserWarning\u001b[39;00m,\n\u001b[0;32m   1094\u001b[0m         )\n\u001b[0;32m   1096\u001b[0m \u001b[39mif\u001b[39;00m y \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m   1097\u001b[0m     \u001b[39m# fit method of arity 1 (unsupervised transformation)\u001b[39;00m\n\u001b[1;32m-> 1098\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfit(X, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mfit_params)\u001b[39m.\u001b[39mtransform(X)\n\u001b[0;32m   1099\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   1100\u001b[0m     \u001b[39m# fit method of arity 2 (supervised transformation)\u001b[39;00m\n\u001b[0;32m   1101\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfit(X, y, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mfit_params)\u001b[39m.\u001b[39mtransform(X)\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\sklearn\\base.py:1474\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1467\u001b[0m     estimator\u001b[39m.\u001b[39m_validate_params()\n\u001b[0;32m   1469\u001b[0m \u001b[39mwith\u001b[39;00m config_context(\n\u001b[0;32m   1470\u001b[0m     skip_parameter_validation\u001b[39m=\u001b[39m(\n\u001b[0;32m   1471\u001b[0m         prefer_skip_nested_validation \u001b[39mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1472\u001b[0m     )\n\u001b[0;32m   1473\u001b[0m ):\n\u001b[1;32m-> 1474\u001b[0m     \u001b[39mreturn\u001b[39;00m fit_method(estimator, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:1495\u001b[0m, in \u001b[0;36mOrdinalEncoder.fit\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m   1488\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\n\u001b[0;32m   1489\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39munknown_value should only be set when \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1490\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mhandle_unknown is \u001b[39m\u001b[39m'\u001b[39m\u001b[39muse_encoded_value\u001b[39m\u001b[39m'\u001b[39m\u001b[39m, \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1491\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mgot \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39munknown_value\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1492\u001b[0m     )\n\u001b[0;32m   1494\u001b[0m \u001b[39m# `_fit` will only raise an error when `self.handle_unknown=\"error\"`\u001b[39;00m\n\u001b[1;32m-> 1495\u001b[0m fit_results \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_fit(\n\u001b[0;32m   1496\u001b[0m     X,\n\u001b[0;32m   1497\u001b[0m     handle_unknown\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mhandle_unknown,\n\u001b[0;32m   1498\u001b[0m     force_all_finite\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mallow-nan\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m   1499\u001b[0m     return_and_ignore_missing_for_infrequent\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[0;32m   1500\u001b[0m )\n\u001b[0;32m   1501\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_missing_indices \u001b[39m=\u001b[39m fit_results[\u001b[39m\"\u001b[39m\u001b[39mmissing_indices\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[0;32m   1503\u001b[0m cardinalities \u001b[39m=\u001b[39m [\u001b[39mlen\u001b[39m(categories) \u001b[39mfor\u001b[39;00m categories \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcategories_]\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:78\u001b[0m, in \u001b[0;36m_BaseEncoder._fit\u001b[1;34m(self, X, handle_unknown, force_all_finite, return_counts, return_and_ignore_missing_for_infrequent)\u001b[0m\n\u001b[0;32m     76\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_n_features(X, reset\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m     77\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_feature_names(X, reset\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m---> 78\u001b[0m X_list, n_samples, n_features \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_check_X(\n\u001b[0;32m     79\u001b[0m     X, force_all_finite\u001b[39m=\u001b[39;49mforce_all_finite\n\u001b[0;32m     80\u001b[0m )\n\u001b[0;32m     81\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_features_in_ \u001b[39m=\u001b[39m n_features\n\u001b[0;32m     83\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcategories \u001b[39m!=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mauto\u001b[39m\u001b[39m\"\u001b[39m:\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:44\u001b[0m, in \u001b[0;36m_BaseEncoder._check_X\u001b[1;34m(self, X, force_all_finite)\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m     32\u001b[0m \u001b[39mPerform custom check_array:\u001b[39;00m\n\u001b[0;32m     33\u001b[0m \u001b[39m- convert list of strings to object dtype\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     40\u001b[0m \n\u001b[0;32m     41\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m     42\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mhasattr\u001b[39m(X, \u001b[39m\"\u001b[39m\u001b[39miloc\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mand\u001b[39;00m \u001b[39mgetattr\u001b[39m(X, \u001b[39m\"\u001b[39m\u001b[39mndim\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m0\u001b[39m) \u001b[39m==\u001b[39m \u001b[39m2\u001b[39m):\n\u001b[0;32m     43\u001b[0m     \u001b[39m# if not a dataframe, do normal check_array validation\u001b[39;00m\n\u001b[1;32m---> 44\u001b[0m     X_temp \u001b[39m=\u001b[39m check_array(X, dtype\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m, force_all_finite\u001b[39m=\u001b[39;49mforce_all_finite)\n\u001b[0;32m     45\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mhasattr\u001b[39m(X, \u001b[39m\"\u001b[39m\u001b[39mdtype\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mand\u001b[39;00m np\u001b[39m.\u001b[39missubdtype(X_temp\u001b[39m.\u001b[39mdtype, np\u001b[39m.\u001b[39mstr_):\n\u001b[0;32m     46\u001b[0m         X \u001b[39m=\u001b[39m check_array(X, dtype\u001b[39m=\u001b[39m\u001b[39mobject\u001b[39m, force_all_finite\u001b[39m=\u001b[39mforce_all_finite)\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\sklearn\\utils\\validation.py:1035\u001b[0m, in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[0;32m   1028\u001b[0m         \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   1029\u001b[0m             msg \u001b[39m=\u001b[39m (\n\u001b[0;32m   1030\u001b[0m                 \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mExpected 2D array, got 1D array instead:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39marray=\u001b[39m\u001b[39m{\u001b[39;00marray\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1031\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39mReshape your data either using array.reshape(-1, 1) if \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1032\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39myour data has a single feature or array.reshape(1, -1) \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1033\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39mif it contains a single sample.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1034\u001b[0m             )\n\u001b[1;32m-> 1035\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(msg)\n\u001b[0;32m   1037\u001b[0m \u001b[39mif\u001b[39;00m dtype_numeric \u001b[39mand\u001b[39;00m \u001b[39mhasattr\u001b[39m(array\u001b[39m.\u001b[39mdtype, \u001b[39m\"\u001b[39m\u001b[39mkind\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mand\u001b[39;00m array\u001b[39m.\u001b[39mdtype\u001b[39m.\u001b[39mkind \u001b[39min\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mUSV\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m   1038\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m   1039\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mdtype=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mnumeric\u001b[39m\u001b[39m'\u001b[39m\u001b[39m is not compatible with arrays of bytes/strings.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1040\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mConvert your data to numeric values explicitly instead.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1041\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: Expected a 2-dimensional container but got <class 'pandas.core.series.Series'> instead. Pass a DataFrame containing a single row (i.e. single sample) or a single column (i.e. single feature) instead."
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "\n",
    "df_encode = df_filter_specific.copy()\n",
    "# Basement Ordinal Encoding\n",
    "valid_basements = ['0', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K']\n",
    "basements_encoder = OrdinalEncoder(categories=[valid_basements])\n",
    "df_encode['basements_encoded'] = basements_encoder.fit_transform(df_encode[['basements']])\n",
    "df_encode = df_encode.drop(columns=['basements'])\n",
    "\n",
    "# # Exterior Condition Encoding\n",
    "# valid_exterior_conditions = sorted(df_encode['exterior_condition'].unique().astype(int), reverse=True)\n",
    "# exterior_encoder = OrdinalEncoder(categories=[valid_exterior_conditions])\n",
    "# df['exterior_encoded'] = exterior_encoder.fit_transform(df_encode['exterior_condition'])\n",
    "# df_encode = df_encode.drop(columns=['exterior_condition'])\n",
    "\n",
    "# # Valid Interior Condition Encoding\n",
    "# valid_interior_condition = ['2', '3', '4', '5', '6', '7']\n",
    "# interior_encoder = OrdinalEncoder(categories=[valid_interior_condition])\n",
    "# df['interior_encoded'] = exterior_encoder.fit_transform(df_encode['interior_condition'])\n",
    "# df_encode = df_encode.drop(columns=['interior_condition'])\n",
    "\n",
    "# # Type Heater Encoding\n",
    "# valid_type_heater = ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H']\n",
    "# heater_encoder = OrdinalEncoder(categories=[valid_type_heater])\n",
    "# df_encode['type_heater_encoded'] = heater_encoder.fit_transform(df_encode[['type_heater']])\n",
    "# df_encode = df_encode.drop(columns=['type_heater'])\n",
    "\n",
    "# df_encode.to_csv('fil.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>basements</th>\n",
       "      <th>census_tract</th>\n",
       "      <th>depth</th>\n",
       "      <th>exempt_building</th>\n",
       "      <th>exterior_condition</th>\n",
       "      <th>fireplaces</th>\n",
       "      <th>frontage</th>\n",
       "      <th>garage_spaces</th>\n",
       "      <th>geographic_ward</th>\n",
       "      <th>homestead_exemption</th>\n",
       "      <th>...</th>\n",
       "      <th>taxable_building</th>\n",
       "      <th>taxable_land</th>\n",
       "      <th>topography</th>\n",
       "      <th>total_area</th>\n",
       "      <th>total_livable_area</th>\n",
       "      <th>type_heater</th>\n",
       "      <th>view_type</th>\n",
       "      <th>year_built</th>\n",
       "      <th>zip_code</th>\n",
       "      <th>zoning</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>0 rows Ã— 28 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [basements, census_tract, depth, exempt_building, exterior_condition, fireplaces, frontage, garage_spaces, geographic_ward, homestead_exemption, interior_condition, market_value, number_of_bathrooms, number_of_bedrooms, number_stories, parcel_shape, street_designation, street_name, taxable_building, taxable_land, topography, total_area, total_livable_area, type_heater, view_type, year_built, zip_code, zoning]\n",
       "Index: []\n",
       "\n",
       "[0 rows x 28 columns]"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_filter_specific"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
