{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dines\\AppData\\Local\\Temp\\ipykernel_7228\\313849615.py:114: DtypeWarning: Columns (5,26,77) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv('original_dataset.csv')\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def drop_high_missing_percent_columns(df):\n",
    "    # Drop columns with more than 25% missing values\n",
    "    df = df.dropna(thresh=len(df) * 0.25, axis=1)\n",
    "    return df\n",
    "\n",
    "def drop_high_cardinality_columns(df):\n",
    "    # Drop columns with too many distinct values as we want to form clusters within the dataset\n",
    "    # and create relationships between the features. These distinct values within a single column for\n",
    "    # every single record blurs the relationships, and as such we are removing it.\n",
    "    # The entire dataset has about 580k rows at this point, so we are targeting columns with 20% distinct values (116k distinct values)\n",
    "    high_cardinality_columns = [col for col in df.columns if df[col].nunique() > 116_000]\n",
    "    # The high_cardinality columns are: ['the_geom', 'the_geom_webmercator', 'book_and_page', 'location', 'parcel_number', 'registry_number', 'pin', 'objectid', 'lat', 'lng']\n",
    "    # We want to preserve lat,lng so we remove it from the list\n",
    "    df = df.drop(columns=high_cardinality_columns)\n",
    "    return df\n",
    "\n",
    "def filter_single_multifamily_homes(df):\n",
    "    # Filter out only homes (single / multi-family homes)\n",
    "    df = df[(df['category_code_description'] == \"SINGLE FAMILY\")]\n",
    "    return df\n",
    "\n",
    "def drop_specific(df):\n",
    "    # Dropped in order to avoid recency bias\n",
    "    df = df.drop(columns=['assessment_date'])\n",
    "\n",
    "    # There's only a single category: Single Family Home(s) and both of these columns have redundant information of the same single value representing single-family home(s).\n",
    "    df = df.drop(columns=['category_code'])\n",
    "    df = df.drop(columns=['category_code_description'])\n",
    "\n",
    "    # This column only re-assures us if the year built has been estimated and doesn't really provide us value towards estimating an valuation of the property.\n",
    "    # More valuable alternative is the year_built column which also has much less missing values.\n",
    "    df = df.drop(columns=['year_built_estimate'])\n",
    "\n",
    "    # Dropped in order to avoid recency bias\n",
    "    df = df.drop(columns=['recording_date'])\n",
    "    # Drop columns: 'mailing_city_state', 'mailing_zip' b/c the mailing address / state of a property owner doesn't indicate a property's worth.\n",
    "    df = df.drop(columns=['mailing_city_state', 'mailing_zip'])\n",
    "\n",
    "    # Dropped building_code because it was not possible to find a clear definition of what the codes had represented.\n",
    "    # There is no updated code manual nor is it even described within the metadata of the Office of Phildelphia's Assessment \n",
    "    df = df.drop(columns=['building_code'])\n",
    "\n",
    "    # Dropped b/c water department uses this as some form of identification number that was not clarified by the Office of Philadelphia's metadata.\n",
    "    df = df.drop(columns=['street_code'])\n",
    "\n",
    "    # Too many missing values and near impossible to impute. It is simply a nominal attribute that is hard to attribute towards our target variable and very susceptible\n",
    "    # to forming bad patterns within our model such as north direction could increase market valuation, but the zipcode and lat/lng are better indicators of an area based\n",
    "    # valuation.\n",
    "    df = df.drop(columns=['street_direction'])\n",
    "    \n",
    "    # Unknown definition of two-digit numbers, weren't even listed in the OPA's metadata\n",
    "    df = df.drop(columns=['building_code_new'])\n",
    "\n",
    "    # Remove all Vacant Land Properties from the dataset\n",
    "    df = df[~df['building_code_description'].str.contains(\"VACANT\", regex=False, na=False)]\n",
    "\n",
    "    # Remove records with placeholder values for sale_price and market_value\n",
    "    df = df[df['sale_price'] > 1]\n",
    "    df = df[df['market_value'] > 1]\n",
    "    \n",
    "    # Have ensured at this point most of the dataset is primarily of single-family homes.\n",
    "    df = df.drop(columns=['building_code_description'])\n",
    "    df = df.drop(columns=['building_code_description_new'])\n",
    "    \n",
    "    # Removed Central Air as it's a binary variable with no other meaningful reference to impute from (38% missing)\n",
    "    df = df.drop(columns=['central_air'])\n",
    "\n",
    "    # Unclear Meaning from Metadata\n",
    "    df = df.drop(columns=['off_street_open'])\n",
    "\n",
    "    # Dropping State Code as this information isn't pertinent to relations of the market value within Philadelphia\n",
    "    df = df.drop(columns=['state_code'])\n",
    "\n",
    "    # Dropping House Number as these are based more on the local neighborhoods within Philadelphia which tends to have very little value in regards to market_value\n",
    "    df = df.drop(columns=['house_number'])\n",
    "\n",
    "    # Nearly 12k missing values and there's unclear definition on what is an average and what each letter represents for general construction from the metadata\n",
    "    df = df.drop(columns=['general_construction'])\n",
    "\n",
    "    # Unclear Definitions and too many messy values\n",
    "    df = df.drop(columns=['quality_grade'])\n",
    "\n",
    "    # Dropped due to low correlation to market value: 0.06\n",
    "    df = df.drop(columns=['exempt_land'])\n",
    "\n",
    "    # Dropped due to too wide spread when manually analyzing\n",
    "    df = df.drop(columns=['sale_price'])\n",
    "\n",
    "    return df\n",
    "\n",
    "def impute_columns(df):\n",
    "    # Imputed Missing Values in basement column with new attribute K.\n",
    "    df = df.fillna({'basements': \"K\"})\n",
    "\n",
    "    # Imputed Missing Values in type_heated column with attribute H (represents missing/unknown heating type for property).\n",
    "    df = df.fillna({'type_heater': \"H\"})\n",
    "\n",
    "    # Imputed Missing Values in topography column with attribute F (represents street level as most properties in philadelphia are at street level according OPA).\n",
    "    df = df.fillna({'topography': \"F\"})\n",
    "\n",
    "    return df\n",
    "\n",
    "def drop_missing_vals_records(df):\n",
    "    df = df.dropna(subset=['census_tract', 'depth', 'exterior_condition', 'fireplaces', 'frontage', 'garage_spaces', \n",
    "                           'geographic_ward', 'interior_condition', 'market_value', 'number_of_bathrooms', 'number_of_bedrooms', \n",
    "                           'number_stories', 'parcel_shape', 'taxable_building', 'total_area', 'total_livable_area', 'view_type',\n",
    "                            'year_built', 'zip_code', 'zoning'])\n",
    "\n",
    "    return df\n",
    "    \n",
    "# Load original_dataset from Office of Property Assessments\n",
    "df = pd.read_csv('original_dataset.csv')\n",
    "\n",
    "df_clean_missing = drop_high_missing_percent_columns(df.copy())\n",
    "df_clean_high_cardinality = drop_high_cardinality_columns(df_clean_missing.copy())\n",
    "df_filter_single_family_homes = filter_single_multifamily_homes(df_clean_high_cardinality.copy())\n",
    "df_remove_specific = drop_specific(df_filter_single_family_homes.copy())\n",
    "df_impute_columns = impute_columns(df_remove_specific.copy())\n",
    "df_remove_missing_vals_records = drop_missing_vals_records(df_impute_columns.copy())\n",
    "\n",
    "# Define a function to filter dates before 2024\n",
    "def filter_dates(row):\n",
    "    year = int(row['sale_date'][:4])\n",
    "    return year < 2024\n",
    "\n",
    "# Apply the filter function to the DataFrame and drop the column as we want to avoid recency bias\n",
    "df_filter_saledate = df_remove_missing_vals_records[df_remove_missing_vals_records.apply(filter_dates, axis=1)].drop(columns=['sale_date'])\n",
    "\n",
    "def filter_specific(df):\n",
    "    # Constraining Basements to the following valid indexes and transform to ordinal encoding\n",
    "    valid_basement = ['0', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K']\n",
    "    df = df[df['basements'].isin(valid_basement)]\n",
    "\n",
    "    # Constraining type_heater to valid definitions from metadata\n",
    "    valid_type_heater = ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H']\n",
    "    df = df[df['type_heater'].isin(valid_type_heater)]\n",
    "\n",
    "    # Constraining view_type to valid definitions from metadata\n",
    "    valid_view_types = ['I', 'H', 'D', 'A', 'C', '0', 'E', 'B']\n",
    "    df = df[df['view_type'].isin(valid_view_types)]\n",
    "\n",
    "    # Constraining topography to valid definitions from metadata\n",
    "    valid_topography_types = ['A', 'B', 'C', 'D', 'E', 'F']\n",
    "    df = df[df['topography'].isin(valid_topography_types)]\n",
    "    \n",
    "    # Constraining parcel_shape to valid definitions from metadata\n",
    "    valid_parcel_shape = ['A', 'B', 'C', 'D', 'E']\n",
    "    df = df[df['parcel_shape'].isin(valid_parcel_shape)]\n",
    "    \n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "df_filter_specific = filter_specific(df_filter_saledate.copy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                     homestead_exemption  market_value\n",
      "homestead_exemption             1.000000      0.135084\n",
      "market_value                    0.135084      1.000000\n",
      "                             homestead_exemption_encoded  market_value\n",
      "homestead_exemption_encoded                     1.000000      0.123055\n",
      "market_value                                    0.123055      1.000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python311\\Lib\\site-packages\\category_encoders\\ordinal.py:198: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  X[column] = X[column].astype(\"object\").fillna(np.nan).map(col_mapping)\n",
      "c:\\Python311\\Lib\\site-packages\\category_encoders\\ordinal.py:198: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  X[column] = X[column].astype(\"object\").fillna(np.nan).map(col_mapping)\n",
      "c:\\Python311\\Lib\\site-packages\\category_encoders\\ordinal.py:198: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  X[column] = X[column].astype(\"object\").fillna(np.nan).map(col_mapping)\n",
      "c:\\Python311\\Lib\\site-packages\\category_encoders\\ordinal.py:198: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  X[column] = X[column].astype(\"object\").fillna(np.nan).map(col_mapping)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from category_encoders import BinaryEncoder\n",
    "from category_encoders import TargetEncoder\n",
    "\n",
    "df_encode = df_filter_specific.copy()\n",
    "# Basement Ordinal Encoding\n",
    "valid_basements = ['0', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K']\n",
    "basements_encoder = OrdinalEncoder(categories=[valid_basements])\n",
    "df_encode['basements_encoded'] = basements_encoder.fit_transform(df_encode[['basements']])\n",
    "df_encode = df_encode.drop(columns=['basements'])\n",
    "\n",
    "# Exterior Condition Ordinal Encoding\n",
    "valid_exterior_conditions = sorted(df_encode['exterior_condition'].unique().astype(int))\n",
    "exterior_encoder = OrdinalEncoder(categories=[valid_exterior_conditions])\n",
    "df_encode['exterior_encoded'] = exterior_encoder.fit_transform(df_encode[['exterior_condition']])\n",
    "df_encode = df_encode.drop(columns=['exterior_condition'])\n",
    "\n",
    "# Interior Condition Ordinal Encoding\n",
    "valid_interior_conditions = sorted(df_encode['interior_condition'].unique().astype(int))\n",
    "interior_encoder = OrdinalEncoder(categories=[valid_interior_conditions])\n",
    "df_encode['interior_encoded'] = interior_encoder.fit_transform(df_encode[['interior_condition']])\n",
    "# Remove records with invalid interior condition that do not have any definitions within metadata OPA.\n",
    "df_encode = df_encode[df_encode['interior_encoded'] != 0]\n",
    "df_encode = df_encode[df_encode['interior_encoded'] != 1]\n",
    "df_encode = df_encode[df_encode['interior_encoded'] != 8]\n",
    "df_encode = df_encode.drop(columns=['interior_condition'])\n",
    "\n",
    "# Type Heater Ordinal Encoding\n",
    "valid_type_heater = ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H']\n",
    "heater_encoder = OrdinalEncoder(categories=[valid_type_heater])\n",
    "df_encode['type_heater_encoded'] = heater_encoder.fit_transform(df_encode[['type_heater']])\n",
    "df_encode = df_encode.drop(columns=['type_heater'])\n",
    "\n",
    "# View Type One Hot Encode Encoding\n",
    "view_encoder = OneHotEncoder(drop=None, sparse_output=False)\n",
    "view_encoder.fit(df_encode[['view_type']])\n",
    "df_encode[['view_type_I', 'view_type_H', 'view_type_D', 'view_type_A', 'view_type_C', 'view_type_0', 'view_type_E', 'view_type_B']] = view_encoder.transform(df_encode[['view_type']])\n",
    "\n",
    "# Topograhy One Hot Encode (as there is no order and we don't want to influence priority)\n",
    "topography_encoder = OneHotEncoder(drop=None, sparse_output=False)\n",
    "topography_encoder.fit(df_encode[['topography']])\n",
    "df_encode[['topography_A', 'topography_B', 'topography_C', 'topography_D', 'topography_E', 'topography_F']] = topography_encoder.transform(df_encode[['topography']])\n",
    "\n",
    "# Parcel Shape One Hot Encode (as there is no order and we don't want to influence priority)\n",
    "parcel_shape_encoder = OneHotEncoder(drop=None, sparse_output=False)\n",
    "parcel_shape_encoder.fit(df_encode[['parcel_shape']])\n",
    "df_encode[['parcel_shape_A', 'parcel_shape_B', 'parcel_shape_C', 'parcel_shape_D', 'parcel_shape_E']] = parcel_shape_encoder.transform(df_encode[['parcel_shape']])\n",
    "\n",
    "# Homestead Exemption\n",
    "print(df_encode[['homestead_exemption', 'market_value']].corr())\n",
    "df_encode['homestead_exemption_encoded'] = df_encode['homestead_exemption'].clip(0, 1)\n",
    "print(df_encode[['homestead_exemption_encoded', 'market_value']].corr())\n",
    "df_encode.drop(columns=['homestead_exemption'])\n",
    "\n",
    "\n",
    "# Zoning (35 different zoning, and nominal attribute, we are going to binary encode this column)\n",
    "# Binary Encoded in the following order: ['RSA5' 'RSA3' 'RM1' 'RMX2' 'RSD3' 'RM4' 'CA1' 'RSA2' 'RSD1' 'CMX4' 'CMX5'\n",
    "#  'CMX2' 'RM2' 'RSA4' 'RSA1' 'ICMX' 'RM3' 'RMX3' 'RTA1' 'RMX1' 'I3' 'IRMX'\n",
    "#  'CMX1' 'CMX3' 'RSD2' 'I2' 'RSA6' 'I1' 'CMX2.5' 'SPINS' 'SPPOA'\n",
    "#  'RSD1|RSD3' 'ICMX|SPPOA' 'CA2' 'RSA5|RSA5']\n",
    "zoning_encoder = BinaryEncoder(cols=['zoning'])\n",
    "df_encode = zoning_encoder.fit_transform(df_encode)\n",
    "\n",
    "# Zipcode Target Encoding\n",
    "zipcode_encoder = TargetEncoder(cols=['zip_code'])\n",
    "df_encode = zipcode_encoder.fit_transform(df_encode['zip_code'], df_encode['market_value'])\n",
    "\n",
    "\n",
    "# Street Name\n",
    "\n",
    "# Street Designation\n",
    "\n",
    "\n",
    "# df_encode.to_csv('fil.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['RSA5' 'RSA3' 'RM1' 'RMX2' 'RSD3' 'RM4' 'CA1' 'RSA2' 'RSD1' 'CMX4' 'CMX5'\n",
      " 'CMX2' 'RM2' 'RSA4' 'RSA1' 'ICMX' 'RM3' 'RMX3' 'RTA1' 'RMX1' 'I3' 'IRMX'\n",
      " 'CMX1' 'CMX3' 'RSD2' 'I2' 'RSA6' 'I1' 'CMX2.5' 'SPINS' 'SPPOA'\n",
      " 'RSD1|RSD3' 'ICMX|SPPOA' 'CA2' 'RSA5|RSA5']\n"
     ]
    }
   ],
   "source": [
    "print(df_encode['zoning'].unique())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
